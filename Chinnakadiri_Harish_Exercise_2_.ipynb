{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarishChinnakadiri/Harish_INFO5731_Spring2024/blob/main/Chinnakadiri_Harish_Exercise_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "#Research Objective:\n",
        "''''Does the length of the warranty for a product affect its customer satisfaction scores in the electronics department of e-commerce platforms?\n",
        "\n",
        "This research tends to look for the presence of a relationship between the warranty period offered for electronic\n",
        "products—smartphones, laptops, tablets, and home appliances—and satisfaction levels expressed by customers through ratings.\n",
        "It postulates that higher satisfaction levels may be related to extended warranty periods as an indication of consumer satisfaction with product\n",
        "reliability and self-confidence of the manufacturer.\n",
        "\n",
        "Data Need to be collected\n",
        "\n",
        "Product Name: For individual product identification purposes.\n",
        "Type/Specification: Grouping products according to their features or specifications.\n",
        "Warranty Info: To know how long a particular product is under warranty\n",
        "Discount Price: Knowing what the price would be after the discount has been made.\n",
        "Original Price: Compare with discounted price to compute discount amount/percentage.'''\n",
        "\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "40eaaa97-a4be-41df-8e9e-088804c618f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"'Does the length of the warranty for a product affect its customer satisfaction scores in the electronics department of e-commerce platforms?\\n\\nThis research tends to look for the presence of a relationship between the warranty period offered for electronic \\nproducts—smartphones, laptops, tablets, and home appliances—and satisfaction levels expressed by customers through ratings. \\nIt postulates that higher satisfaction levels may be related to extended warranty periods as an indication of consumer satisfaction with product \\nreliability and self-confidence of the manufacturer.\\n\\nData Need to be collected\\n\\nProduct Name: For individual product identification purposes.\\nType/Specification: Grouping products according to their features or specifications.\\nWarranty Info: To know how long a particular product is under warranty\\nDiscount Price: Knowing what the price would be after the discount has been made.\\nOriginal Price: Compare with discounted price to compute discount amount/percentage.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "# write your answer here\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "# Initialize empty lists to store the data\n",
        "product_names = []\n",
        "product_specs = []\n",
        "product_warranties = []\n",
        "sale_prices = []\n",
        "list_prices = []\n",
        "\n",
        "# Loop through a range of pages (here, only one page for demonstration)\n",
        "for page_number in range(1, 2):\n",
        "    # Construct the URL for the page\n",
        "    url = f'https://www.backmarket.com/en-us/l/smartphones/0744fd27-8605-465d-8691-3b6dffda5969?page={page_number}#'\n",
        "\n",
        "    # Send a GET request to the website\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = bs(response.content, 'html.parser')\n",
        "\n",
        "    # Find all product data elements on the page\n",
        "    product_list = soup.find_all('div', {'class': 'flex flex-col md:flex-1 md:justify-end'})\n",
        "\n",
        "    # Iterate through the product elements to extract information\n",
        "    for product in product_list:\n",
        "        try:\n",
        "            # Get the product name\n",
        "            name = product.find('h2').get_text()\n",
        "        except:\n",
        "            name = 'N/A'\n",
        "\n",
        "        try:\n",
        "            # Get the product type/specification\n",
        "            spec = product.find('span', {'class': 'body-2-light duration-200 line-clamp-1 normal-case overflow-ellipsis overflow-hidden text-black transition-all'}).get_text().strip()\n",
        "        except:\n",
        "            spec = 'N/A'\n",
        "\n",
        "        try:\n",
        "            # Get the product warranty information\n",
        "            warranty = product.find('span', {'class': 'body-2-light text-black'}).get_text().strip()\n",
        "        except:\n",
        "            warranty = 'N/A'\n",
        "\n",
        "        try:\n",
        "            # Get the discounted price\n",
        "            discount = product.find('span', {'class': 'body-2-bold text-black'}).get_text().strip()\n",
        "        except:\n",
        "            discount = 'N/A'\n",
        "\n",
        "        try:\n",
        "            # Get the actual price (original price)\n",
        "            original = product.find('div', {'class': 'body-2-light text-primary-light line-through'}).get_text().strip()\n",
        "        except:\n",
        "            original = 'N/A'\n",
        "\n",
        "        # Append the extracted data to respective lists\n",
        "        product_names.append(name)\n",
        "        product_specs.append(spec)\n",
        "        product_warranties.append(warranty)\n",
        "        sale_prices.append(discount)\n",
        "        list_prices.append(original)\n",
        "\n",
        "# Create a DataFrame to store the extracted data\n",
        "df = pd.DataFrame({'Product Name': product_names, 'Specification': product_specs, 'Warranty': product_warranties,\n",
        "                   'Sale Price': sale_prices, 'List Price': list_prices})\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "excel_file = \"updated_extracted_data.xlsx\"\n",
        "df.to_excel(excel_file, index=False)\n",
        "\n",
        "# Display the DataFrame and the Excel filename\n",
        "print(f\"Data has been saved to {excel_file}\")\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d82b23f-0be6-4a46-bd3a-a512ec145085"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to updated_extracted_data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4045929e-2888-43aa-afd9-0a1bbad402c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 0 articles and saved to 'articles.xlsx'.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "\n",
        "# Function to fetch articles from Google Scholar\n",
        "def fetch_google_scholar_articles(query, start_year, end_year, num_articles):\n",
        "    # Base URL for Google Scholar\n",
        "    base_url = \"https://scholar.google.com/scholar?hl=en&as_sdt=0,44&q=web+scraping\"\n",
        "    articles = []  # List to store the collected articles\n",
        "    articles_per_page = 10  # Adjusted based on actual Google Scholar pagination\n",
        "\n",
        "    # Loop to paginate through search results\n",
        "    for start in range(0, num_articles, articles_per_page):\n",
        "        # Parameters for the search query, including keywords and date range\n",
        "        params = {\n",
        "            \"q\": query,             # Search query\n",
        "            \"as_ylo\": start_year,   # Start year of publication range\n",
        "            \"as_yhi\": end_year,     # End year of publication range\n",
        "            \"start\": start          # Pagination offset\n",
        "        }\n",
        "\n",
        "        # User-Agent header to mimic a web browser\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.1234.0 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "        # Send a GET request to the Google Scholar search URL with parameters and headers\n",
        "        response = requests.get(base_url, params=params, headers=headers)\n",
        "\n",
        "        # Check if the response is successful (HTTP status code 200)\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content of the response using BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all the search result div elements\n",
        "            results = soup.find_all('div', {'class': 'gs_ri'})\n",
        "\n",
        "            # Iterate through each search result\n",
        "            for result in results:\n",
        "                article = {}  # Dictionary to store article information\n",
        "\n",
        "                # Extract the title, venue, authors, year, and abstract from the results\n",
        "                title = result.find('h3', {'class': 'gs_rt'}).text if result.find('h3', {'class': 'gs_rt'}) else \"N/A\"\n",
        "                venue = result.find('div', {'class': 'gs_a'}).text.split('-')[1].strip() if result.find('div', {'class': 'gs_a'}) else \"N/A\"\n",
        "                authors_year_info = result.find('div', {'class': 'gs_a'}).text if result.find('div', {'class': 'gs_a'}) else \"N/A\"\n",
        "                abstract = result.find('div', {'class': 'gs_rs'}).text if result.find('div', {'class': 'gs_rs'}) else \"N/A\"\n",
        "\n",
        "                # Store extracted information in the article dictionary\n",
        "                article['title'] = title\n",
        "                article['venue'] = venue\n",
        "                article['authors'] = authors_year_info.split('-')[0].strip()\n",
        "                article['year'] = authors_year_info.split('-')[-1].strip().split(',')[0].strip()\n",
        "                article['abstract'] = abstract\n",
        "\n",
        "                # Append the article dictionary to the list of articles\n",
        "                articles.append(article)\n",
        "\n",
        "                # Break the loop if the desired number of articles is reached\n",
        "                if len(articles) >= num_articles:\n",
        "                    break\n",
        "\n",
        "            # Break the outer loop if the desired number of articles is reached\n",
        "            if len(articles) >= num_articles:\n",
        "                break\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Main program\n",
        "if __name__ == \"__main__\":\n",
        "    keyword = \"information retrieval\"  # Keyword for the search\n",
        "    start_year = 2000               # Start year of publication range\n",
        "    end_year = 2024                # End year of publication range\n",
        "    num_articles = 500              # Desired number of articles to collect, adjusted to 500 as per your request\n",
        "\n",
        "    # Call the fetch_google_scholar_articles function to collect articles\n",
        "    articles = fetch_google_scholar_articles(keyword, start_year, end_year, num_articles)\n",
        "\n",
        "    # Convert the list of dictionaries to a pandas DataFrame\n",
        "    df_articles = pd.DataFrame(articles)\n",
        "\n",
        "    # Specify the filename for the Excel file\n",
        "    excel_filename = \"articles.xlsx\"\n",
        "\n",
        "    # Save the DataFrame to an Excel file\n",
        "    df_articles.to_excel(excel_filename, index=False)\n",
        "\n",
        "    # Print the number of collected articles and a confirmation message\n",
        "    print(f\"Collected {len(articles)} articles and saved to '{excel_filename}'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actually i have got articles from the same code when i fisrt run . After 2 tyms i run the code and  not able to extract articles. After some reasearch i came to know that using same IP address for multiple times may block the user temporarily due to security reasons."
      ],
      "metadata": {
        "id": "OY6du3vAym1V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I57NXsauCec2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0572bd-5a2c-4b23-93e5-fab94002aba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://github.com/HarishChinnakadiri/Harish_INFO5731_Spring2024/blob/main/Parse%20Hub.pdf\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "A=('https://github.com/HarishChinnakadiri/Harish_INFO5731_Spring2024/blob/main/Parse%20Hub.pdf')\n",
        "print(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Key concepts and techniques that I found most beneficial in understanding the process of web scraping include:\n",
        "1. Understanding the HTML code is very crucial for web scrapping. Finding the HTML tags, attributes which helps in identifying the data to be extracted.\n",
        "Utilizing libraries and frameworks such as BeautifulSoup in Python simplifies the process of parsing HTML documents and extracting relevant data.\n",
        "Familiarity with these tools streamlines the implementation of web scraping solutions.\n",
        "It's essential to be mindful of the website's policies and practice ethical web scraping.\n",
        "2. While webscrapping from the googlescholar , scemantic and others even though i have an valid API.\n",
        "I cannot able to extract articles because Google Scholar is known for its strict anti-scraping measures. Frequent requests from the same IP address can quickly lead to temporary blocks,\n",
        "There were many restrictions to the  websites.\n",
        "In the field of data science, gathering and analyzing online data is like having a superpower. It lets data scientists find patterns, understand trends, and make predictions by looking at a huge amount of information from the internet. This can help in making smart decisions, spotting what customers like, and even predicting future events. It's all about turning the massive sea of online data into valuable insights that can guide actions in business, health, science, and more. But, it's also important to use this power responsibly, respecting people's privacy and following the rules.\n",
        "As a whole I do feel that tym provided for this assignment is too low . Because there are many restrictions for each website eventhough we have an API. It taught me a lot while doing this assignment.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "0b377d21-e6e5-420a-ad29-6e9285a6be63"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nKey concepts and techniques that I found most beneficial in understanding the process of web scraping include:\\n1. Understanding the HTML code is very crucial for web scrapping. Finding the HTML tags, attributes which helps in identifying the data to be extracted.\\nUtilizing libraries and frameworks such as BeautifulSoup in Python simplifies the process of parsing HTML documents and extracting relevant data.\\nFamiliarity with these tools streamlines the implementation of web scraping solutions.\\nIt's essential to be mindful of the website's policies and practice ethical web scraping. \\n2. While webscrapping from the googlescholar , scemantic and others even though i have an valid API.\\nI cannot able to extract articles because Google Scholar is known for its strict anti-scraping measures. Frequent requests from the same IP address can quickly lead to temporary blocks, \\nThere were many restrictions to the  websites.\\nIn the field of data science, gathering and analyzing online data is like having a superpower. It lets data scientists find patterns, understand trends, and make predictions by looking at a huge amount of information from the internet. This can help in making smart decisions, spotting what customers like, and even predicting future events. It's all about turning the massive sea of online data into valuable insights that can guide actions in business, health, science, and more. But, it's also important to use this power responsibly, respecting people's privacy and following the rules.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}