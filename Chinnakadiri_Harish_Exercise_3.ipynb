{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarishChinnakadiri/Harish_INFO5731_Spring2024/blob/main/Chinnakadiri_Harish_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Sentiment analysis of movie reviews: This task involves classifying the sentiment of movie reviews as positive, negative, or neutral.\n",
        "\n",
        "Features:\n",
        "Word Frequency (Bag of Words): Counts of individual words can signal positive or negative sentiment. Frequent positive words indicate a positive review and vice versa.\n",
        "\n",
        "TF-IDF Scores: Highlights important words that are frequent in a document but rare across all documents, helping to identify key sentiment drivers.\n",
        "\n",
        "N-grams: Sequences of n-words provide context beyond single words, capturing phrases that have specific sentiment implications (e.g., \"not good\").\n",
        "\n",
        "Sentiment Scores: Precomputed sentiment scores for words/phrases can aggregate to determine overall sentiment of the text.\n",
        "\n",
        "Part-of-Speech Tags: Adjectives and adverbs are often tied to sentiment (e.g., \"amazing\" or \"terribly\"), making them useful for sentiment analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c9f6f7-5404-4748-fdc9-b03bd35fe601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Bag-of-Words (BoW) specific to Movie Terminology:\n",
            "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]]\n",
            "\n",
            "2. TF-IDF with Movie-Related Vocabulary:\n",
            "[[1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.70710678 0.70710678 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.70710678 0.70710678 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         1.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.57735027 0.57735027 0.57735027 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.57735027 0.57735027 0.57735027\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.70710678 0.70710678 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         1.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.70710678 0.70710678]]\n",
            "\n",
            "3. N-grams:\n",
            "Shape: (10, 260)\n",
            "\n",
            "4. Sentiment Analysis:\n",
            "[0.5, -0.6, 0.7833333333333333, 0.15, 0.5, -0.24621212121212122, -0.15000000000000002, 0.4, 0.3375, 0.27499999999999997]\n",
            "\n",
            "5. Document Structure and Length:\n",
            "[11, 12, 8, 14, 8, 14, 11, 9, 11, 11]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "\n",
        "# Sample text data (movie reviews)\n",
        "reviews = [\n",
        "    \"An outstanding narrative that keeps you engaged from start to finish.\",\n",
        "    \"The plot was predictable and lacked depth, making it a boring watch.\",\n",
        "    \"Exceptional cinematography and brilliant performances by the cast.\",\n",
        "    \"The movie's pacing was off, dragging in the middle but with an exciting climax.\",\n",
        "    \"A masterpiece in storytelling with a captivating soundtrack.\",\n",
        "    \"Failed to live up to the hype, with underdeveloped characters and a weak storyline.\",\n",
        "    \"A visual treat, but the plot twists felt forced and unnatural.\",\n",
        "    \"Engaging and thought-provoking, a must-watch for any film enthusiast.\",\n",
        "    \"The director's unique vision comes to life in this compelling drama.\",\n",
        "    \"More style than substance, it looks good but ultimately feels empty.\"\n",
        "]\n",
        "\n",
        "# Adjusted movie-related terms for focused feature extraction\n",
        "movie_related_terms = ['outstanding', 'predictable', 'depth', 'cinematography', 'performances', 'pacing', 'masterpiece', 'storytelling', 'soundtrack', 'hype', 'characters', 'storyline', 'visual', 'twists', 'engaging', 'director', 'style', 'substance']\n",
        "\n",
        "# Extracting Bag-of-Words (BoW) features\n",
        "vectorizer_bow = CountVectorizer(vocabulary=movie_related_terms)\n",
        "bow_features = vectorizer_bow.fit_transform(reviews).toarray()\n",
        "\n",
        "# Extracting TF-IDF features\n",
        "vectorizer_tfidf = TfidfVectorizer(vocabulary=movie_related_terms)\n",
        "tfidf_features = vectorizer_tfidf.fit_transform(reviews).toarray()\n",
        "\n",
        "# Extracting N-grams features\n",
        "vectorizer_ngram = CountVectorizer(ngram_range=(1, 3))\n",
        "ngram_features = vectorizer_ngram.fit_transform(reviews).toarray()\n",
        "\n",
        "# Performing sentiment analysis\n",
        "sentiments = [TextBlob(review).sentiment.polarity for review in reviews]\n",
        "\n",
        "# Calculating document lengths\n",
        "lengths = [len(review.split()) for review in reviews]\n",
        "\n",
        "# Printing structured output\n",
        "print(\"1. Bag-of-Words (BoW) specific to Movie Terminology:\")\n",
        "print(np.array(bow_features))\n",
        "\n",
        "print(\"\\n2. TF-IDF with Movie-Related Vocabulary:\")\n",
        "print(np.array(tfidf_features))\n",
        "\n",
        "print(\"\\n3. N-grams:\")\n",
        "# For brevity, only displaying the shape as the full matrix would be large\n",
        "print(f\"Shape: {ngram_features.shape}\")\n",
        "\n",
        "print(\"\\n4. Sentiment Analysis:\")\n",
        "print(sentiments)\n",
        "\n",
        "print(\"\\n5. Document Structure and Length:\")\n",
        "print(lengths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca4b7ae-e52c-4475-8de0-3cbad3ff12eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked features based on TF-IDF scores for all words:\n",
            "1. the: 1.2370\n",
            "2. and: 1.0177\n",
            "3. to: 0.9151\n",
            "4. in: 0.7934\n",
            "5. with: 0.7710\n",
            "6. but: 0.7180\n",
            "7. plot: 0.5932\n",
            "8. watch: 0.5732\n",
            "9. it: 0.5562\n",
            "10. was: 0.5444\n",
            "11. an: 0.5186\n",
            "12. captivating: 0.4425\n",
            "13. masterpiece: 0.4425\n",
            "14. soundtrack: 0.4425\n",
            "15. storytelling: 0.4425\n",
            "16. brilliant: 0.3881\n",
            "17. by: 0.3881\n",
            "18. cast: 0.3881\n",
            "19. cinematography: 0.3881\n",
            "20. exceptional: 0.3881\n",
            "21. performances: 0.3881\n",
            "22. felt: 0.3554\n",
            "23. forced: 0.3554\n",
            "24. treat: 0.3554\n",
            "25. twists: 0.3554\n",
            "26. unnatural: 0.3554\n",
            "27. visual: 0.3554\n",
            "28. boring: 0.3424\n",
            "29. depth: 0.3424\n",
            "30. lacked: 0.3424\n",
            "31. making: 0.3424\n",
            "32. predictable: 0.3424\n",
            "33. any: 0.3319\n",
            "34. engaging: 0.3319\n",
            "35. enthusiast: 0.3319\n",
            "36. film: 0.3319\n",
            "37. for: 0.3319\n",
            "38. must: 0.3319\n",
            "39. provoking: 0.3319\n",
            "40. thought: 0.3319\n",
            "41. comes: 0.3263\n",
            "42. compelling: 0.3263\n",
            "43. director: 0.3263\n",
            "44. drama: 0.3263\n",
            "45. life: 0.3263\n",
            "46. this: 0.3263\n",
            "47. unique: 0.3263\n",
            "48. vision: 0.3263\n",
            "49. empty: 0.3120\n",
            "50. engaged: 0.3120\n",
            "51. feels: 0.3120\n",
            "52. finish: 0.3120\n",
            "53. from: 0.3120\n",
            "54. good: 0.3120\n",
            "55. keeps: 0.3120\n",
            "56. looks: 0.3120\n",
            "57. more: 0.3120\n",
            "58. narrative: 0.3120\n",
            "59. outstanding: 0.3120\n",
            "60. start: 0.3120\n",
            "61. style: 0.3120\n",
            "62. substance: 0.3120\n",
            "63. than: 0.3120\n",
            "64. that: 0.3120\n",
            "65. ultimately: 0.3120\n",
            "66. you: 0.3120\n",
            "67. climax: 0.2980\n",
            "68. dragging: 0.2980\n",
            "69. exciting: 0.2980\n",
            "70. middle: 0.2980\n",
            "71. movie: 0.2980\n",
            "72. off: 0.2980\n",
            "73. pacing: 0.2980\n",
            "74. characters: 0.2961\n",
            "75. failed: 0.2961\n",
            "76. hype: 0.2961\n",
            "77. live: 0.2961\n",
            "78. storyline: 0.2961\n",
            "79. underdeveloped: 0.2961\n",
            "80. up: 0.2961\n",
            "81. weak: 0.2961\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Sample text data (movie reviews)\n",
        "reviews = [\n",
        "    \"An outstanding narrative that keeps you engaged from start to finish.\",\n",
        "    \"The plot was predictable and lacked depth, making it a boring watch.\",\n",
        "    \"Exceptional cinematography and brilliant performances by the cast.\",\n",
        "    \"The movie's pacing was off, dragging in the middle but with an exciting climax.\",\n",
        "    \"A masterpiece in storytelling with a captivating soundtrack.\",\n",
        "    \"Failed to live up to the hype, with underdeveloped characters and a weak storyline.\",\n",
        "    \"A visual treat, but the plot twists felt forced and unnatural.\",\n",
        "    \"Engaging and thought-provoking, a must-watch for any film enthusiast.\",\n",
        "    \"The director's unique vision comes to life in this compelling drama.\",\n",
        "    \"More style than substance, it looks good but ultimately feels empty.\"\n",
        "]\n",
        "\n",
        "# Using TF-IDF to extract features\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer_tfidf.fit_transform(reviews)\n",
        "feature_names = vectorizer_tfidf.get_feature_names_out()\n",
        "\n",
        "# Summing up the TF-IDF scores for each term across all documents\n",
        "sums = tfidf_matrix.sum(axis=0)\n",
        "data = [(term, sums[0, col]) for col, term in enumerate(feature_names)]\n",
        "\n",
        "# Sorting the terms by their summed TF-IDF score in descending order\n",
        "ranking = sorted(data, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Displaying the ranked features based on their TF-IDF scores for all words\n",
        "print(\"Ranked features based on TF-IDF scores for all words:\")\n",
        "for rank, (term, score) in enumerate(ranking, start=1):\n",
        "    print(f\"{rank}. {term}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d463ef-ffb1-41d8-fb0f-2724cc000562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked texts based on similarity to the query:\n",
            "Text: An outstanding narrative that keeps you engaged from start to finish., Similarity Score: 0.8701763153076172\n",
            "Text: A masterpiece in storytelling with a captivating soundtrack., Similarity Score: 0.7749614715576172\n",
            "Text: Engaging and thought-provoking, a must-watch for any film enthusiast., Similarity Score: 0.7745383381843567\n",
            "Text: The director's unique vision comes to life in this compelling drama., Similarity Score: 0.7662754058837891\n",
            "Text: The movie's pacing was off, dragging in the middle but with an exciting climax., Similarity Score: 0.6782228946685791\n",
            "Text: A visual treat, but the plot twists felt forced and unnatural., Similarity Score: 0.6712093949317932\n",
            "Text: More style than substance, it looks good but ultimately feels empty., Similarity Score: 0.6626472473144531\n",
            "Text: The plot was predictable and lacked depth, making it a boring watch., Similarity Score: 0.6510769724845886\n",
            "Text: Exceptional cinematography and brilliant performances by the cast., Similarity Score: 0.6458926796913147\n",
            "Text: Failed to live up to the hype, with underdeveloped characters and a weak storyline., Similarity Score: 0.6363062262535095\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to encode texts using BERT to get embeddings\n",
        "def get_bert_embeddings(texts):\n",
        "    # Tokenize and encode the texts with padding and truncation\n",
        "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "    # Get the model output (last hidden states)\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded_input)\n",
        "    # Get the embeddings from the last hidden state by averaging across token embeddings\n",
        "    embeddings = output.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "# Sample text data (movie reviews) from question 2\n",
        "texts = [\n",
        "    \"An outstanding narrative that keeps you engaged from start to finish.\",\n",
        "    \"The plot was predictable and lacked depth, making it a boring watch.\",\n",
        "    \"Exceptional cinematography and brilliant performances by the cast.\",\n",
        "    \"The movie's pacing was off, dragging in the middle but with an exciting climax.\",\n",
        "    \"A masterpiece in storytelling with a captivating soundtrack.\",\n",
        "    \"Failed to live up to the hype, with underdeveloped characters and a weak storyline.\",\n",
        "    \"A visual treat, but the plot twists felt forced and unnatural.\",\n",
        "    \"Engaging and thought-provoking, a must-watch for any film enthusiast.\",\n",
        "    \"The director's unique vision comes to life in this compelling drama.\",\n",
        "    \"More style than substance, it looks good but ultimately feels empty.\"\n",
        "]\n",
        "\n",
        "# Your query\n",
        "query = \"A compelling story that captures your attention.\"\n",
        "\n",
        "# Encode the query and texts to get their embeddings\n",
        "query_embedding = get_bert_embeddings([query])\n",
        "text_embeddings = get_bert_embeddings(texts)\n",
        "\n",
        "# Calculate cosine similarities between the query and each text\n",
        "similarities = [cosine_similarity(query_embedding, text_embedding.unsqueeze(0)) for text_embedding in text_embeddings]\n",
        "\n",
        "# Rank the texts based on similarity scores in descending order\n",
        "ranked_indices = sorted(range(len(texts)), key=lambda i: similarities[i], reverse=True)\n",
        "\n",
        "# Display the ranked texts with their similarity scores\n",
        "print(\"Ranked texts based on similarity to the query:\")\n",
        "for index in ranked_indices:\n",
        "    print(f\"Text: {texts[index]}, Similarity Score: {similarities[index].item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "User\n",
        "In exploring feature extraction from text data, I've found a blend of techniques invaluable for navigating the complexities of natural language processing. Bag of Words (BoW) and TF-IDF provide a solid foundation, translating text into numerical vectors that capture word frequency and significance. The evolution towards word and contextual embeddings, notably through models like Word2Vec, GloVe, and BERT, marked a significant leap, offering nuanced, semantic representations of text. Cosine similarity emerged as a crucial tool for assessing text similarities, enhancing the relevance of document comparisons. Lastly, feature selection techniques proved essential in optimizing model performance, highlighting the importance of identifying and focusing on the most informative features.\n",
        "Completing feature extraction exercises from text data often involves challenges like managing high-dimensional data, capturing contextual nuances, and executing effective data preprocessing. Advanced models like BERT improve accuracy but demand more computational power.\n",
        "This exercise is central to the field of Natural Language Processing (NLP), focusing on transforming raw text into structured, analyzable data. It underscores the importance of feature extraction and representation, pivotal for tasks like sentiment analysis, text classification, and semantic similarity, thereby enhancing machine understanding and processing of human language.\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}